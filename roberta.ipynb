{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd_DxhQ3wrhq"
      },
      "source": [
        "NLP | SENTIMENT DETECTION USING ROBERTA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moq33tJFv5ZM"
      },
      "source": [
        "#IMPORTING REQUIRED LIBRARIES\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtcz3PSzwhAD"
      },
      "source": [
        "#Training data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "print('Training data shape: ', train.shape)\n",
        "print('Testing data shape: ', test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Ev3sbgwpfp"
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCDATSSUw7qH"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4e743xRw-kO"
      },
      "source": [
        "#missing value checking\n",
        "\n",
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pVIllV4xM0r"
      },
      "source": [
        "# we are dropping these rows\n",
        "train.dropna(axis = 0, how ='any',inplace=True)\n",
        "train.reset_index(inplace=True)\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WObCZ4hcxS_L"
      },
      "source": [
        "#Function to print height of barcharts on the bars\n",
        "\n",
        "def barh(ax): # for getting height   \n",
        "    for p in ax.patches:\n",
        "        val = p.get_height() #height of the bar\n",
        "        x = p.get_x()+ p.get_width()/2 # x- position \n",
        "        y = p.get_y() + p.get_height()/2 #y-position\n",
        "        ax.annotate(round(val,2),(x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pK00XbXxXcy"
      },
      "source": [
        "#checking distribution of sentiment values in train data \n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "ax0 = sns.countplot(train['sentiment'],order=train['sentiment'].value_counts().index)\n",
        "barh(ax0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hClqXRxdxb_b"
      },
      "source": [
        "#checking distribution of sentiment values in test data\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "ax1 = sns.countplot(test['sentiment'],order=test['sentiment'].value_counts().index)\n",
        "barh(ax1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkk6X26xxn7e"
      },
      "source": [
        "#separating text according to sentiments:\n",
        "\n",
        "positive_text = train[train['sentiment'] == 'positive']['selected_text']\n",
        "negative_text = train[train['sentiment'] == 'negative']['selected_text']\n",
        "neutral_text = train[train['sentiment'] == 'neutral']['selected_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1hwQl4CxuVG"
      },
      "source": [
        "#text cleaning functions\n",
        "\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower() #lowercase\n",
        "    text = re.sub('\\[.*?\\]', '', text) #removing anything written inside []\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)# removing urls\n",
        "    text = re.sub('<.*?>+', '', text) #removing anything written inside <>\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #removing punctuations\n",
        "    text = re.sub('\\n', '', text) #removing '\\n' (new lines)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text) #removing anything that is like hello34zero ,i.e. any string that has a digits inside\n",
        "    return text\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    Cleaning and parsing the text.\n",
        "\n",
        "    \"\"\"\n",
        "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "    nopunc = clean_text(text)\n",
        "    tokenized_text = tokenizer.tokenize(nopunc)\n",
        "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
        "    combined_text = ' '.join(tokenized_text)\n",
        "    return combined_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2S-nyEIxzLv"
      },
      "source": [
        "#cleaning text\n",
        "positive_text_clean = positive_text.apply(lambda x: text_preprocessing(x))\n",
        "negative_text_clean = negative_text.apply(lambda x: text_preprocessing(x))\n",
        "neutral_text_clean = neutral_text.apply(lambda x: text_preprocessing(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vUMggCYx51F"
      },
      "source": [
        "#importing word cloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "plt.figure(figsize = (20,7))\n",
        "wordcloud1 = WordCloud( background_color='white',\n",
        "                        width=600,\n",
        "                        height=400).generate(\" \".join(positive_text_clean))\n",
        "plt.imshow(wordcloud1)\n",
        "plt.axis('off')\n",
        "plt.title('Positive text',fontsize=30);\n",
        "\n",
        "plt.figure(figsize = (20,7))\n",
        "wordcloud2 = WordCloud( background_color='white',\n",
        "                        width=600,\n",
        "                        height=400).generate(\" \".join(negative_text_clean))\n",
        "plt.imshow(wordcloud2)\n",
        "plt.axis('off')\n",
        "plt.title('Negative text',fontsize=30);\n",
        "\n",
        "plt.figure(figsize = (20,7))\n",
        "wordcloud3 = WordCloud( background_color='white',\n",
        "                        width=600,\n",
        "                        height=400).generate(\" \".join(neutral_text_clean))\n",
        "plt.imshow(wordcloud3)\n",
        "plt.axis('off')\n",
        "plt.title('Neutral text',fontsize=30);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXO5yyvXyCiH"
      },
      "source": [
        "#Encoding Train data for Roberta\n",
        "#tokenizer\n",
        "PATH = '/tf-roberta/'\n",
        "\n",
        "#ByteLevelBPETokenizer has some additional convenience features like lowercase=True and addprefixspace=True.Hence we are using this\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=PATH+'vocab-roberta-base.json', \n",
        "    merges_file=PATH+'merges-roberta-base.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2jKnhtSyLmg"
      },
      "source": [
        "# calculating length of the longest text\n",
        "\n",
        "MAX_LEN = 0\n",
        "\n",
        "for text in train['text']:\n",
        "\n",
        "    # Tokenize the text and add special tokens i.e [`<s>]` and `[SEP]`\n",
        "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    MAX_LEN = max(MAX_LEN, len(input_ids))\n",
        "\n",
        "print('Max length: ', MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e-HMFnZyPmP"
      },
      "source": [
        "#sentiment encoding\n",
        "\n",
        "sent = ['positive','negative','neutral']\n",
        "sent_id =[]\n",
        "for i in sent:\n",
        "    sent_id.append(tokenizer.encode(i).ids[0])\n",
        "sentiment_id = dict(zip(sent,sent_id))\n",
        "sentiment_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsNvwGz8yZcw"
      },
      "source": [
        "#creating RoBERTa format inputs (input_ids,attention_mark,start_tokens,end_tokens)\n",
        "\n",
        "ct = train.shape[0]\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(train.shape[0]):\n",
        "    \n",
        "    # Finding overlap between text and selected_text\n",
        "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "#     print(k)\n",
        "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==' ':\n",
        "        chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1) \n",
        "#     print(enc)\n",
        "        \n",
        "    # ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "    \n",
        "    # START & END TOKENS\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
        "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask[k,:len(enc.ids)+5] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+1] = 1\n",
        "        end_tokens[k,toks[-1]+1] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPofHTDnyelo"
      },
      "source": [
        "#Encoding test data\n",
        "ct = test.shape[0]\n",
        "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(test.shape[0]):\n",
        "        \n",
        "    # INPUT_IDS\n",
        "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "    enc = tokenizer.encode(text1)                \n",
        "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
        "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask_t[k,:len(enc.ids)+5] = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSBp-LC8ymjm"
      },
      "source": [
        "#Build roBERTa Model (Custom question answer machine)\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    #we have to target variables (start_token and end_token). hence we are creating two models to classify values(1 or 0) for these variables\n",
        "    \n",
        "    '''\n",
        "    The Conv1D are essential because they preserve spatial information. We want our model to predict a start index and end index which are spatial.\n",
        "    If you use GlobalAveragePooling1D you will lose spatial information. You will know which texts are positive, negative, neutral but you won't \n",
        "    know where the words are located.\n",
        "    '''\n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qJHgVviyt-U"
      },
      "source": [
        "#defining the metric\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    if (len(a)==0) & (len(b)==0): return 0.5\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ZYBKFmzCSr"
      },
      "source": [
        "Training :\n",
        "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "        \n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='auto', save_freq='epoch') #to save best weights \n",
        "        \n",
        "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
        "        epochs=5, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [start_tokens[idxV,], end_tokens[idxV,]])) \n",
        "    \n",
        "    print('Loading model...')\n",
        "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    print('Predicting Test...')\n",
        "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/skf.n_splits\n",
        "    preds_end += preds[1]/skf.n_splits\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNzHPdQ7zJ7D"
      },
      "source": [
        "Predicting:\n",
        "result= []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "    if a>b: \n",
        "        st = test.loc[k,'text']\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        st = tokenizer.decode(enc.ids[a-1:b])\n",
        "    result.append(st)\n",
        "result[:5]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}